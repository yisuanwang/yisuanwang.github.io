
# üìù Publications 

## üßë‚Äçüé® 3D AIGC

<!-- [DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters](https://driveavatar.github.io/) -->
<div class='paper-box'>
<div class='paper-box-image'>
    <div class="badge">arxiv 2024</div>
      <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
           onclick="window.open('https://driveavatar.github.io/', '_blank');">
      <source src="images/pub_DRiVE.mp4" type="video/mp4">
    </video>
</div>

<div class='paper-box-text' markdown="1">


<h1 style="font-weight: bold">
  <a href="https://driveavatar.github.io/" target="_blank" style="text-decoration: none; color: inherit;">
    <span class="gradient-text-drive">DRiVE</span>:
    <span style="text-decoration: underline; text-decoration-skip-ink: none;">D</span>iffusion-based 
    <span style="text-decoration: underline; text-decoration-skip-ink: none;">Ri</span>gging Empowers Generation of 
    <span style="text-decoration: underline; text-decoration-skip-ink: none;">V</span>ersatile and 
    <span style="text-decoration: underline; text-decoration-skip-ink: none;">E</span>xpressive Characters
  </a>
</h1>



[Mingze Sun *](https://scholar.google.com/citations?hl=en&user=TTW2mVoAAAAJ), 
[**<font color="#fc8803">Junhao Chen *</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
[Junting Dong ‚Ä†](https://scholar.google.com/citations?user=dEzL5pAAAAAJ&hl=en&oi=ao), 
Yurun Chen, Xinyu Jiang, Shiwei Mao,
[Puhua Jiang](https://scholar.google.com/citations?user=E-k3WcgAAAAJ&hl=en), 
[Jingbo Wang](https://scholar.google.com/citations?user=GStTsxAAAAAJ&hl=en), 
[Bo Dai](https://scholar.google.com/citations?hl=en&user=KNWTvgEAAAAJ), 
[Ruqi Huang ‚Ä†](https://scholar.google.com/citations?user=cgRY63gAAAAJ&hl=en)

[\[üóÇÔ∏èProject Page\]](https://driveavatar.github.io/) 
[![GitHub Repo Stars](https://img.shields.io/github/stars/DRiVEAvatar/DRiVEAvatar.github.io?label=stars&logo=github&color=brightgreen)](https://github.com/DRiVEAvatar/DRiVEAvatar.github.io)
<!-- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1u_lJRvxIlBUPjC_Lou57SWLEnc5vLgQ6?usp=sharing) -->
[![arXiv](https://img.shields.io/badge/arXiv-2411.17423-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2411.17423)

  - This work generates skeleton and skinning with clothes and hair for 3d gaussian avatar!

</div>
</div>



<!-- Idea23D -->
<div class='paper-box'>
<div class='paper-box-image'>
    <div class="badge">arxiv 2024</div>
    <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
           onclick="window.open('https://air-discover.github.io/Idea-2-3D/', '_blank');">
      <source src="images/pub_idea23d.mp4" type="video/mp4">
    </video>
</div>
<div class='paper-box-text' markdown="1">
<h1 style="font-weight: bold">
  <a href="https://air-discover.github.io/Idea-2-3D/" target="_blank">
    <span class="gradient-text-idea23d">Idea-2-3D</span>:
      Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs
  </a>
</h1>


[**<font color="#fc8803">Junhao Chen *</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
[Xiang Li *](https://scholar.google.com/citations?user=_wyYvQsAAAAJ&hl=zh-CN), 
[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
Chao Li, 
[Zhaoxin Fan ‚Ä†](https://scholar.google.com/citations?user=JHvyYDQAAAAJ), 
[Hao Zhao ‚Ä†](https://scholar.google.com/citations?hl=en&user=ygQznUQAAAAJ)


[\[üóÇÔ∏èProject Page\]](https://air-discover.github.io/Idea-2-3D/) 
[![GitHub Repo Stars](https://img.shields.io/github/stars/yisuanwang/Idea23D?label=stars&logo=github&color=brightgreen)](https://github.com/yisuanwang/Idea23D)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1u_lJRvxIlBUPjC_Lou57SWLEnc5vLgQ6?usp=sharing)
[![arXiv](https://img.shields.io/badge/arXiv-2404.04363-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2404.04363)

  - This work enables automated 3D model design and generation for people!

</div>
</div>


<!-- Ultraman -->
<div class='paper-box'>
<div class='paper-box-image'>
    <div class="badge">arxiv 2024</div>
    <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
           onclick="window.open('https://air-discover.github.io/Ultraman/', '_blank');">
      <source src="images/pub_ultraman_Compressed.mp4" type="video/mp4">
    </video>
</div>

<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  <a href="https://air-discover.github.io/Ultraman/" target="_blank">
    <b style="color: #5a3e91">Ultra</b><b style="color: #d73d5f">man</b>: Single Image 3D Human Reconstruction with Ultra Speed and Detail
  </a>
</h1>

[Mingjin Chen *](https://scholar.google.com/citations?user=uLfubbgAAAAJ&hl=en&oi=sra), 
[**<font color="#fc8803">Junhao Chen *</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
[Huan-ang Gao](https://scholar.google.com/citations?hl=en&user=WvbKfLgAAAAJ), 
[Xiaoxue Chen](https://scholar.google.com/citations?hl=en&user=_tz64W0AAAAJ), 
[Zhaoxin Fan](https://scholar.google.com/citations?user=JHvyYDQAAAAJ), 
[Hao Zhao ‚Ä†](https://scholar.google.com/citations?hl=en&user=ygQznUQAAAAJ)

[\[üóÇÔ∏èProject Page\]](https://air-discover.github.io/Ultraman/) 
[![GitHub Repo Stars](https://img.shields.io/github/stars/tomorrow1238/Ultraman?label=stars&logo=github&color=brightgreen)](https://github.com/tomorrow1238/Ultraman) 
[![arXiv](https://img.shields.io/badge/arXiv-2403.12028-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2403.12028)

  - This work converts a single image of the human body into a lifelike 3D model!
</div>
</div>





## üëÄ Multi-modal

<!--IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web-->

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">arxiv 2024</div>
      <img src="images/pub_iwbench.svg" alt="sym" width="100%" 
           style="cursor: pointer;" 
           onclick="window.open('https://iw-bench-page.vercel.app/', '_blank');">
    </div>
  </div>

<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  <a href="https://iw-bench-page.vercel.app/" target="_blank">
    IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web
  </a>
</h1>


[Hongcheng Guo](https://scholar.google.com/citations?hl=en&user=eynbo4cAAAAJ), 
[Wei Zhang](https://scholar.google.com/citations?user=NaWMztYAAAAJ&hl=en&oi=sra), 
[**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
Yaonan Gu,
[Jian Yang](https://scholar.google.com/citations?user=i9opWEgAAAAJ&hl=en), Junjia Du, Binyuan Hui, 
[Tianyu Liu](https://scholar.google.com/citations?user=6hHbBwwAAAAJ&hl=en), 
[Jianxin Ma](https://scholar.google.com/citations?hl=en&user=WdDFFlIAAAAJ), 
[Chang Zhou](https://scholar.google.com/citations?hl=en&user=QeSoG3sAAAAJ)

[\[üóÇÔ∏èProject Page\]](https://iw-bench-page.vercel.app/) 
[![GitHub Repo Stars](https://img.shields.io/github/stars/HC-Guo/IWBench?label=stars&logo=github&color=brightgreen)](https://github.com/HC-Guo/IWBench) 
[![arXiv](https://img.shields.io/badge/arXiv-2409.18980-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2409.18980)

  - This work is a benchmark for evaluating MLLM image-2-html code generation capabilities.
</div>
</div>



<!--COLING 2024 MMADÔºöMulti-modal Movie Audio Description -->
<div class="paper-box">
  <!-- Paper Box Image Section -->
  <div class="paper-box-image">
    <div>
      <div class="badge">COLING 2024</div>
      <!-- Clickable Image with Link -->
      <img src="images/pub_mmad.png" alt="sym" width="100%" 
           style="cursor: pointer;" 
           onclick="window.open('https://daria8976.github.io/mmad-page/', '_blank');">
    </div>
  </div>
<div class='paper-box-text' markdown="1">
<h1 style="font-weight: bold">
  <a href="https://daria8976.github.io/mmad-page/" target="_blank">
    <span class="gradient-text-MMAD">MMAD</span>:
      Multi-modal Movie Audio Description
  </a>
</h1>


[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
[**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
[Xiang Li](https://scholar.google.com/citations?user=_wyYvQsAAAAJ&hl=zh-CN), 
[Haidong Xin](https://xhd0728.github.io/), 
Chao Li,
[Sheng Zhou ‚Ä†](https://scholar.google.com/citations?user=Ss76nMwAAAAJ&hl=zh-CN), 
[Jiajun Bu](https://scholar.google.com/citations?user=OgZP2okAAAAJ&hl=en)


[\[üóÇÔ∏èProject Page\]](https://daria8976.github.io/mmad-page/) 
[![GitHub Repo Stars](https://img.shields.io/github/stars/Daria8976/MMAD?label=stars&logo=github&color=brightgreen)](https://github.com/Daria8976/MMAD) 
[\[üìúPaper\]](https://aclanthology.org/2024.lrec-main.998/)

  - This work has unlocked a whole new experience of watching movies for the visually impaired.
</div>
</div>




<!-- Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv 2023</div><img src='images/pub_soulstyler.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object
</h1>

[**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
 Peng Rong, Jingbo Sun, Chao Li ‚Ä†, Xiang Li, [Hongwu Lv](http://homepage.hrbeu.edu.cn/web/lvhongwu?locale=zh_CN) 

[![GitHub Repo Stars](https://img.shields.io/github/stars/yisuanwang/Soulstyler?label=stars&logo=github&color=brightgreen)](https://github.com/yisuanwang/Soulstyler) 
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1cn4W7IlooDk5X9JXBvsENRtExKJShb98#scrollTo=F0LyDZnKoTuT) 
[![arXiv](https://img.shields.io/badge/arXiv-2311.13562-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2311.13562)

  - This work enables fine-grained stylization of a single image through text-guidance!

</div>
</div>


## üéô NLP & LLM
<!-- ICANN 2023 Towards Energy-Efficient Sentiment Classification with Spiking Neural Networks -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICANN 2023</div><img src='images/pub_spike.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  Towards Energy-Efficient Sentiment Classification with Spiking Neural Networks
</h1>

[**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), Jingbo Sun, Chao Li ‚Ä†

[\[üìúPaper\]](https://doi.org/10.1007/978-3-031-44204-9_43)

  - This work applies a pulsed neural network to a natural language sentiment categorization task, reaching the leading edge in terms of energy consumption.

</div>
</div>




<!-- EMNLP 2023 ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2023</div><img src='images/pub_zhujiu.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models
</h1>

[Baoli Zhang \*](https://scholar.google.com/citations?hl=en&user=ZUW0UbgAAAAJ), Haining Xie \*, Pengfan Du,
[**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
[Pengfei Cao](https://cpf-nlpr.github.io/), [Yubo Chen ‚Ä†](https://people.ucas.ac.cn/~yubochen), Shengping Liu, [Kang Liu](https://people.ucas.ac.cn/~liukang), [Jun Zhao](https://people.ucas.ac.cn/~zhaojun)

[\[üóÇÔ∏èProject Page\]](http://www.zhujiu-benchmark.com/introduction)
[\[üèÜLeaderboard \]](http://www.zhujiu-benchmark.com/leaderboard)
[![arXiv](https://img.shields.io/badge/arXiv-2308.14353-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2308.14353)
[\[üìúPaper\]](https://aclanthology.org/2023.emnlp-demo.44/)
[\[üé•Video\]](https://www.youtube.com/watch?v=qypkJ89L1Ic)

  - This work serves as a benchmark for evaluating the Chinese language capabilities of large language models.
</div>
</div>


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2022</div><img src='images/diffsinger.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism](https://arxiv.org/abs/2105.02446) \\
Jinglin Liu, Chengxi Li, **Yi Ren**, Feiyang Chen, Zhou Zhao

- Many [video demos](https://www.bilibili.com/video/BV1be411N7JA) created by the [DiffSinger community](https://github.com/openvpi) are released.
- DiffSinger was introduced in [a very popular video](https://www.bilibili.com/video/BV1uM411t7ZJ) (1600k+ views) on Bilibili!

- [**Project**](https://diffsinger.github.io/) \| [![](https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&label=DiffSpeech Stars)](https://github.com/NATSpeech/NATSpeech) \| [![](https://img.shields.io/github/stars/MoonInTheRiver/DiffSinger?style=social&label=DiffSinger Stars)](https://github.com/MoonInTheRiver/DiffSinger) \| [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo)](https://huggingface.co/spaces/NATSpeech/DiffSpeech)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2021</div><img src='images/portaspeech.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[PortaSpeech: Portable and High-Quality Generative Text-to-Speech](https://arxiv.org/abs/2109.15166) \\
**Yi Ren**, Jinglin Liu, Zhou Zhao

[**Project**](https://portaspeech.github.io/) \| [![](https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&label=Code+Stars)](https://github.com/NATSpeech/NATSpeech) \| [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo)](https://huggingface.co/spaces/NATSpeech/PortaSpeech)
</div>
</div> -->

<!-- ## üìÉPatents

| Id | Date     | Name                                           | Number     | Type         |
| ---| -------- | ---------------------------------------------- | --- | -------------- | 
| 6  | 2024-03-22 |  ‰∏ÄÁßçÁõ≤‰∫∫Êô∫ËÉΩÁúºÈïú                             | - | China Utility Model |
| 5  | 2024-03-22 |  ‰∏ÄÁßçÊãºÊé•ÊëÑÂÉèÂ§¥                               | CN220647706U | China Utility Model |
| 4  | 2023-06-21 |  ‰∏ÄÁßçÂü∫‰∫éÁ•ûÁªèÁΩëÁªúÁöÑÊú¨Âú∞ÂåñÁªèÈ™åÂ®ÅËÉÅÂàÜÊûêÊñπÊ≥ïÂèäË£ÖÁΩÆ | CN116962012A | China Invention Publication     |
| 3  | 2023-06-21 |  ‰∏ÄÁßçÂü∫‰∫éÈù¢ÂêëÊµÅÂàáÂàÜÊäÄÊúØÁöÑÁΩëÁªúÊó•ÂøóËß£ÊûêÊñπÊ≥ïÂèäË£ÖÁΩÆ | CN116668154A | China Invention Publication     |
| 2  | 2022-10-31 |  ‰∏ÄÁßçÊï∞ÊéßÊú∫Â∫äÁöÑÂ∫üÊñôÂõûË£ÖÁΩÆ                       | CN218312317U | China Utility Model | 
| 1  | 2021-12-27 |  ‰∏ÄÁßçËà∞ËàπÁî®È´òÂàÜËæ®ÁéáË∂ÖËøúË∑ùÂÖ®ÊôØÊëÑÂÉèÂ§¥             | CN216565344U | China Utility Model |  -->


<!-- invention publicationÔºöÂèëÊòé‰∏ìÂà©Áî≥ËØ∑
invention grantÔºöÂèëÊòé‰∏ìÂà©ÊéàÊùÉ
utility modelÔºöÂÆûÁî®Êñ∞Âûã‰∏ìÂà©ÊéàÊùÉ
designÔºöÂ§ñËßÇËÆæËÆ°‰∏ìÂà©ÊéàÊùÉ
 -->
<!-- ## üìÑSoftware copyrights

| Id | Date       | Name                             | Number        |
| ---- | ------------ | ------------------------------------- | --------------- |
| 7  | 2023-10-25 | TallybookÔºö‰∏ÄÊ¨æËÆ∞Ë¥¶ÁÆ°ÁêÜÁ≥ªÁªü                       | 2023SR1295186 |
| 6  | 2023-08-04 | Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÈ∏üÁ±ªÂ£∞Èü≥ËØÜÂà´Á≥ªÁªü                    | 2023SR0896658 |
| 5  | 2023-08-02 | Ê≥ï‰øù‚Äî‚Äî‰∏ìÊ≥®Â∞èÂæÆ‰ºÅ‰∏öÁöÑÊ≥ïÂæãÂí®ËØ¢Âπ≥Âè∞                | 2023SR0881467 |
| 4  | 2023-05-18 | ÁÅµÈ≠ÇÁîªÊâã‚Äî‚ÄîÊï∞Â≠óÁÖßÁâáËµÑ‰∫ß‰øÆÂ§ç‰∏éÁÆ°ÁêÜËΩØ‰ª∂iOS App     | 2023SR0551686 |
| 3  | 2023-05-18 | Soul PainterÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÂõæÂÉèÂ§ÑÁêÜÊäÄÊúØÁöÑAndroid App | 2023SR0551505 |
| 2  | 2022-05-26 | ËÆ°ÁÆóÊú∫ËßÜËßâÁõÆÊ†áÂÆö‰ΩçÊµãË∑ùÁ≥ªÁªü                        | 2022SR0649919 |
| 1  | 2022-05-26 | Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁâ©‰ΩìËØÜÂà´Â§ÑÁêÜÁ≥ªÁªü                    | 2022SR0649918 | -->
