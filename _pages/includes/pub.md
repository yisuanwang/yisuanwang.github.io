# üìù Publications 

## üßë‚Äçüé® AIGC & Controllable World Model

<!-- DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation -->
<div class='paper-box'>
<div class='paper-box-image'>
    <div class="badge">Arxiv 2025</div>
      <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
           onclick="window.open('https://dancetog.github.io/', '_blank');">
      <source src="images/DanceTog-case01.mp4" type="video/mp4">
    </video>
</div>

<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  <a href="https://dancetog.github.io/" target="_blank" style="text-decoration: none; color: inherit;">
    <img src="images/dancetoglogo.png" alt="DanceTogether logo" style="height: 1em; vertical-align: -0.15em; margin-right: 0.3em;">
    <span class="gradient-text-dance">DanceTogether!</span> Identity-Preserving Multi-Person Interactive Video Generation
  </a>
</h1>

[**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?hl=en&user=uVMnzPMAAAAJ), 
[Mingjin Chen](https://scholar.google.com/), 
[Jianjin Xu](https://scholar.google.com/citations?hl=en&user=mTV0usAAAAAJ), 
[Xiang Li](https://scholar.google.com/citations?user=_wyYvQsAAAAJ&hl=en&oi=sra), 
[Junting Dong ‚Ä†](https://scholar.google.com/citations?user=dEzL5pAAAAAJ&hl=en),
[Mingze Sun](https://scholar.google.com/citations?user=TTW2mVoAAAAJ&hl=en), 
[Puhua Jiang](https://scholar.google.com/citations?user=E-k3WcgAAAAJ&hl=en), 
[Hongxiang Li](https://scholar.google.com/citations?user=U4AwycUAAAAJ&hl=en&oi=ao), 
[Yuhang Yang](https://scholar.google.com/citations?hl=en&user=x3aClGEAAAAJ),
[Hao Zhao](https://scholar.google.com/citations?user=ygQznUQAAAAJ&hl=en), 
[Xiaoxiao Long](https://scholar.google.com/citations?hl=en&user=W3G5kZEAAAAJ), 
[Ruqi Huang ‚Ä†](https://scholar.google.com/citations?user=cgRY63gAAAAJ&hl=en)

<div align="left">
  <a href="https://dancetog.github.io/"><img src="https://img.shields.io/static/v1?label=Homepage&message=DanceTogether&color=blue&logo=github-pages"></a> 
  <a href="https://github.com/yisuanwang/DanceTog"><img src="https://img.shields.io/github/stars/yisuanwang/DanceTog?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a> 
  <a href="https://arxiv.org/abs/2505.18078"><img src="https://img.shields.io/badge/arXiv-2505.18078-b31b1b.svg?style=flat-square" alt="arXiv"></a>
  <!-- <a href="https://huggingface.co/papers/2505.18078"><img src="https://img.shields.io/static/v1?label=Paper&message=HuggingFace&color=yellow"></a> 
  <a href="https://huggingface.co/spaces/BestWishYsh/ChronoMagic-Bench"><img src="https://img.shields.io/static/v1?label=LeaderBoard&message=HuggingFace&color=yellow"></a>  -->
</div>

  - This work generates identity-preserving multi-person interactive dance videos with controllable motion and appearance!

</div>
</div>



<!-- [DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters](https://driveavatar.github.io/) -->
<div class='paper-box'>
<div class='paper-box-image'>
    <div class="badge">CVPR 2025</div>
      <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
           onclick="window.open('https://driveavatar.github.io/', '_blank');">
      <source src="images/pub_DRiVE.mp4" type="video/mp4">
    </video>
</div>

<div class='paper-box-text' markdown="1">


<h1 style="font-weight: bold">
  <a href="https://driveavatar.github.io/" target="_blank" style="text-decoration: none; color: inherit;">
    <span class="gradient-text-drive">DRiVE</span>:
    <span style="text-decoration: underline; text-decoration-skip-ink: none;">D</span>iffusion-based 
    <span style="text-decoration: underline; text-decoration-skip-ink: none;">Ri</span>gging Empowers Generation of 
    <span style="text-decoration: underline; text-decoration-skip-ink: none;">V</span>ersatile and 
    <span style="text-decoration: underline; text-decoration-skip-ink: none;">E</span>xpressive Characters
  </a>
</h1>


[Mingze Sun *](https://scholar.google.com/citations?hl=en&user=TTW2mVoAAAAJ), 
[**<font color="#fc8803">Junhao Chen *</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
[Junting Dong ‚Ä†](https://scholar.google.com/citations?user=dEzL5pAAAAAJ&hl=en&oi=ao), 
Yurun Chen, Xinyu Jiang, Shiwei Mao,
[Puhua Jiang](https://scholar.google.com/citations?user=E-k3WcgAAAAJ&hl=en), 
[Jingbo Wang](https://scholar.google.com/citations?user=GStTsxAAAAAJ&hl=en), 
[Bo Dai](https://scholar.google.com/citations?hl=en&user=KNWTvgEAAAAJ), 
[Ruqi Huang ‚Ä†](https://scholar.google.com/citations?user=cgRY63gAAAAJ&hl=en)

<div align="left">
  <!-- <a href="https://github.com/yisuanwang/DRiVE"><img src="https://img.shields.io/static/v1?label=Code&message=Github&color=blue&logo=github-pages"></a>  -->
  <a href="https://driveavatar.github.io/"><img src="https://img.shields.io/static/v1?label=Homepage&message=DRiVE&color=blue&logo=github-pages"></a> 
  <!-- <a href='https://driveavatar.github.io/'><img src='https://img.shields.io/badge/Project-Page-green'></a>  -->
  <!-- <a href="https://arxiv.org/pdf/2411.02293"><img src="https://img.shields.io/static/v1?label=Tech Report&message=Arxiv&color=red&logo=arxiv"></a>  -->
  <!-- <a href="https://huggingface.co/Tencent/Hunyuan3D-1"><img src="https://img.shields.io/static/v1?label=Checkpoints&message=HuggingFace&color=yellow"></a>  -->
  <!-- <a href="https://huggingface.co/spaces/Tencent/Hunyuan3D-1"><img src="https://img.shields.io/static/v1?label=Demo&message=HuggingFace&color=yellow"></a>  -->
  <a href="https://github.com/yisuanwang/DRiVE"><img src="https://img.shields.io/github/stars/yisuanwang/DRiVE?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a> 
  <a href="https://arxiv.org/abs/2411.17423"><img src="https://img.shields.io/badge/arXiv-2411.17423-b31b1b.svg?style=flat-square" alt="arXiv"></a>
</div>

  - This work generates skeleton and skinning with clothes and hair for 3d gaussian avatar!

</div>
</div>



<!-- Idea23D -->
<div class='paper-box'>
<div class='paper-box-image'>
    <div class="badge">COLING 2025</div>
    <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
           onclick="window.open('https://idea23d.github.io/', '_blank');">
      <source src="images/pub_idea23d.mp4" type="video/mp4">
    </video>
</div>
<div class='paper-box-text' markdown="1">
<h1 style="font-weight: bold">
  <a href="https://idea23d.github.io/" target="_blank">
    <span class="gradient-text-idea23d">Idea23D</span>: Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs
  </a>
</h1>


[**<font color="#fc8803">Junhao Chen *</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
[Xiang Li *](https://scholar.google.com/citations?user=_wyYvQsAAAAJ&hl=zh-CN), 
[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
Chao Li, 
[Zhaoxin Fan ‚Ä†](https://scholar.google.com/citations?user=JHvyYDQAAAAJ), 
[Hao Zhao ‚Ä†](https://scholar.google.com/citations?hl=en&user=ygQznUQAAAAJ)

<!-- <a href='https://idea23d.github.io/'><img src='https://img.shields.io/badge/Project-Page-green'></a>
[![GitHub Repo Stars](https://img.shields.io/github/stars/yisuanwang/Idea23D?label=stars&logo=github&color=brightgreen)](https://github.com/yisuanwang/Idea23D)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1u_lJRvxIlBUPjC_Lou57SWLEnc5vLgQ6?usp=sharing)
[![arXiv](https://img.shields.io/badge/arXiv-2404.04363-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2404.04363) -->

<div align="left">
  <!-- <a href='https://idea23d.github.io/'>
    <img src='https://img.shields.io/badge/Project-Page-green' alt="Project Page">
  </a> -->
  <a href="https://idea23d.github.io/"><img src="https://img.shields.io/static/v1?label=Homepage&message=Idea23D&color=blue&logo=github-pages"></a> 
  <a href="https://github.com/yisuanwang/Idea23D"><img src="https://img.shields.io/github/stars/yisuanwang/Idea23D?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a> 
  <a href="https://colab.research.google.com/drive/1u_lJRvxIlBUPjC_Lou57SWLEnc5vLgQ6?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a> 
  <a href="https://arxiv.org/abs/2404.04363"><img src="https://img.shields.io/badge/arXiv-2404.04363-b31b1b.svg?style=flat-square" alt="arXiv"></a> 
</div>
  - This work enables automated 3D model design and generation for people!

</div>
</div>


<!-- Ultraman -->
<div class='paper-box'>
<div class='paper-box-image'>
    <div class="badge">arxiv 2024</div>
    <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
           onclick="window.open('https://air-discover.github.io/Ultraman/', '_blank');">
      <source src="images/pub_ultraman_Compressed.mp4" type="video/mp4">
    </video>
</div>

<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  <a href="https://air-discover.github.io/Ultraman/" target="_blank">
    <b style="color: #5a3e91">Ultra</b><b style="color: #d73d5f">man</b>: Single Image 3D Human Reconstruction with Ultra Speed and Detail
  </a>
</h1>

[Mingjin Chen *](https://scholar.google.com/citations?user=uLfubbgAAAAJ&hl=en&oi=sra), 
[**<font color="#fc8803">Junhao Chen *</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
[Huan-ang Gao](https://scholar.google.com/citations?hl=en&user=WvbKfLgAAAAJ), 
[Xiaoxue Chen](https://scholar.google.com/citations?hl=en&user=_tz64W0AAAAJ), 
[Zhaoxin Fan](https://scholar.google.com/citations?user=JHvyYDQAAAAJ), 
[Hao Zhao ‚Ä†](https://scholar.google.com/citations?hl=en&user=ygQznUQAAAAJ)


<a href="https://air-discover.github.io/Ultraman/"><img src="https://img.shields.io/static/v1?label=Homepage&message=Ultraman&color=blue&logo=github-pages"></a> 
[![GitHub Repo Stars](https://img.shields.io/github/stars/tomorrow1238/Ultraman?label=stars&logo=github&color=brightgreen)](https://github.com/tomorrow1238/Ultraman) 
[![arXiv](https://img.shields.io/badge/arXiv-2403.12028-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2403.12028)

  - This work converts a single image of the human body into a lifelike 3D model!
</div>
</div>





## üëÄ Multi-modal


<!--IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web-->

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">ACL 2025</div>
      <img src="images/pub_iwbench.svg" alt="sym" width="100%" 
           style="cursor: pointer;" 
           onclick="window.open('https://iw-bench-page.vercel.app/', '_blank');">
    </div>
  </div>

<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  <a href="https://iw-bench-page.vercel.app/" target="_blank">
    IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web
  </a>
</h1>


[Hongcheng Guo](https://scholar.google.com/citations?hl=en&user=eynbo4cAAAAJ), 
[Wei Zhang](https://scholar.google.com/citations?user=NaWMztYAAAAJ&hl=en&oi=sra), 
[**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
Yaonan Gu,
[Jian Yang](https://scholar.google.com/citations?user=i9opWEgAAAAJ&hl=en), Junjia Du, Binyuan Hui, 
[Tianyu Liu](https://scholar.google.com/citations?user=6hHbBwwAAAAJ&hl=en), 
[Jianxin Ma](https://scholar.google.com/citations?hl=en&user=WdDFFlIAAAAJ), 
[Chang Zhou](https://scholar.google.com/citations?hl=en&user=QeSoG3sAAAAJ)


<a href="https://iw-bench-page.vercel.app/"><img src="https://img.shields.io/static/v1?label=Homepage&message=IW-bench&color=blue&logo=github-pages"></a> 
[![GitHub Repo Stars](https://img.shields.io/github/stars/HC-Guo/IWBench?label=stars&logo=github&color=brightgreen)](https://github.com/HC-Guo/IWBench) 
[![arXiv](https://img.shields.io/badge/arXiv-2409.18980-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2409.18980)

  - This work is a benchmark for evaluating MLLM image-2-html code generation capabilities.
</div>
</div>



<!--COLING 2024 MMADÔºöMulti-modal Movie Audio Description -->
<div class="paper-box">
  <!-- Paper Box Image Section -->
  <div class="paper-box-image">
    <div>
      <div class="badge">COLING 2024</div>
      <!-- Clickable Image with Link -->
      <img src="images/pub_mmad.png" alt="sym" width="100%" 
           style="cursor: pointer;" 
           onclick="window.open('https://daria8976.github.io/mmad-page/', '_blank');">
    </div>
  </div>
<div class='paper-box-text' markdown="1">
<h1 style="font-weight: bold">
  <a href="https://daria8976.github.io/mmad-page/" target="_blank">
    <span class="gradient-text-MMAD">MMAD</span>:
      Multi-modal Movie Audio Description
  </a>
</h1>


[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
[**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
[Xiang Li](https://scholar.google.com/citations?user=_wyYvQsAAAAJ&hl=zh-CN), 
[Haidong Xin](https://xhd0728.github.io/), 
Chao Li,
[Sheng Zhou ‚Ä†](https://scholar.google.com/citations?user=Ss76nMwAAAAJ&hl=zh-CN), 
[Jiajun Bu](https://scholar.google.com/citations?user=OgZP2okAAAAJ&hl=en)


<a href="https://daria8976.github.io/mmad-page/"><img src="https://img.shields.io/static/v1?label=Homepage&message=MMAD&color=blue&logo=github-pages"></a> 
[![GitHub Repo Stars](https://img.shields.io/github/stars/Daria8976/MMAD?label=stars&logo=github&color=brightgreen)](https://github.com/Daria8976/MMAD) 
[\[üìúPaper\]](https://aclanthology.org/2024.lrec-main.998/)

  - This work has unlocked a whole new experience of watching movies for the visually impaired.
</div>
</div>




<!-- Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv 2023</div><img src='images/pub_soulstyler.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  FineStyler: Text-guided Instance-level Fine-grained Image Style Transfer
</h1>

[**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
Rong Peng, 
Xiang Li, 
Jingbo Sun, 
Hao Zhao, 
Ruqi Huang

[![GitHub Repo Stars](https://img.shields.io/github/stars/yisuanwang/Soulstyler?label=stars&logo=github&color=brightgreen)](https://github.com/yisuanwang/Soulstyler) 
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1cn4W7IlooDk5X9JXBvsENRtExKJShb98#scrollTo=F0LyDZnKoTuT) 
[![arXiv](https://img.shields.io/badge/arXiv-2311.13562-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2311.13562)

  - This work enables fine-grained stylization of a single image through text-guidance!

</div>
</div>


## üéô NLP & LLM



<!-- ICANN 2023 Towards Energy-Efficient Sentiment Classification with Spiking Neural Networks -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICANN 2023</div><img src='images/pub_spike.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  Towards Energy-Efficient Sentiment Classification with Spiking Neural Networks
</h1>

[**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), Jingbo Sun, Chao Li ‚Ä†

[\[üìúPaper\]](https://doi.org/10.1007/978-3-031-44204-9_43)

  - This work applies a pulsed neural network to a natural language sentiment categorization task, reaching the leading edge in terms of energy consumption.

</div>
</div>




<!-- EMNLP 2023 ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2023</div><img src='images/pub_zhujiu.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models
</h1>

[Baoli Zhang \*](https://scholar.google.com/citations?hl=en&user=ZUW0UbgAAAAJ), Haining Xie \*, Pengfan Du,
[**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
[Pengfei Cao](https://cpf-nlpr.github.io/), [Yubo Chen ‚Ä†](https://people.ucas.ac.cn/~yubochen), Shengping Liu, [Kang Liu](https://people.ucas.ac.cn/~liukang), [Jun Zhao](https://people.ucas.ac.cn/~zhaojun)

<a href="http://www.zhujiu-benchmark.com/introduction"><img src="https://img.shields.io/static/v1?label=Homepage&message=Zhujiu&color=blue&logo=github-pages"></a> 
[\[üèÜLeaderboard \]](http://www.zhujiu-benchmark.com/leaderboard)
[![arXiv](https://img.shields.io/badge/arXiv-2308.14353-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2308.14353)
[\[üìúPaper\]](https://aclanthology.org/2023.emnlp-demo.44/)
[\[üé•Video\]](https://www.youtube.com/watch?v=qypkJ89L1Ic)

  - This work serves as a benchmark for evaluating the Chinese language capabilities of large language models.
</div>
</div>

