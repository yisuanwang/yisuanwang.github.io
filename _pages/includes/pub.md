
# üìù Publications 

## üßë‚Äçüé® 3D AIGC


<!-- Idea23D -->
<div class='paper-box'>

<div class='paper-box-image'>
  <!-- <div class='video-container'> -->
    <div class="badge">arxiv 2024</div>
      <video autoplay="" class="video-style"  loop="" muted="" playsinline="" poster="images/spinner.svg" width="100%">
          <source src="images/pub_idea23d.mp4" type="video/mp4" >
      </video>
  <!-- </div> -->
</div>

<div class='paper-box-text' markdown="1">

<!-- <h1 style="font-weight: bold">
  <a href="https://air-discover.github.io/Idea-2-3D/" target="_blank">
    <b style="color: #5a3e91">Idea</b>-<b style="color: #17bfe9">2</b>-<b style="color: #d73d5f">3D</b>:
      Iterative Self-Refinement with Large Multimodal Model for Automated 3D Model Design and Generation
  </a>
</h1> -->

<h1 style="font-weight: bold">
  <a href="https://air-discover.github.io/Idea-2-3D/" target="_blank">
    <span class="gradient-text">Idea-2-3D</span>:
      Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs
  </a>
</h1>

<!-- <h1 style="font-weight: bold;">
  <a href="https://air-discover.github.io/Idea-2-3D/" target="_blank">
  <span class="gradient-text">Idea-2-3D</span>
  <span>
    : Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs
  </span>
  </a>
</h1> -->

**<font color="#fc8803">Junhao Chen *</font>**, 
[Xiang Li *](https://scholar.google.com/citations?user=_wyYvQsAAAAJ&hl=zh-CN), 
[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
Chao Li, 
[Zhaoxin Fan](https://scholar.google.com/citations?user=JHvyYDQAAAAJ), 
[Hao Zhao ‚Ä†](https://scholar.google.com/citations?hl=en&user=ygQznUQAAAAJ)


[üóÇProject Page](https://air-discover.github.io/Idea-2-3D/) | [![GitHub Repo Stars](https://img.shields.io/github/stars/yisuanwang/Idea23D?label=stars&logo=github&color=brightgreen)](https://github.com/yisuanwang/Idea23D) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1u_lJRvxIlBUPjC_Lou57SWLEnc5vLgQ6?usp=sharing) | [![arXiv](https://img.shields.io/badge/arXiv-2404.04363-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2404.04363)


  - This work enables automated 3D model design and generation for people!

</div>
</div>


<!-- Ultraman -->
<div class='paper-box'>

<div class='paper-box-image'>
  <!-- <div class='video-container'> -->
    <div class="badge">arxiv 2024</div>
      <video autoplay="" class="video-style"  loop="" muted="" playsinline="" poster="images/spinner.svg" width="100%">
          <source src="images/pub_ultraman_Compressed.mp4" type="video/mp4" >
      </video>
  <!-- </div> -->
</div>

<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  <a href="https://air-discover.github.io/Ultraman/" target="_blank">
    <b style="color: #5a3e91">Ultra</b><b style="color: #d73d5f">man</b>: Single Image 3D Human Reconstruction with Ultra Speed and Detail
  </a>
</h1>
  
Mingjin Chen *, 
**<font color="#fc8803">Junhao Chen *</font>**, 
[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
[Huan-ang Gao](https://scholar.google.com/citations?hl=en&user=WvbKfLgAAAAJ), 
[Xiaoxue Chen](https://scholar.google.com/citations?hl=en&user=_tz64W0AAAAJ), 
[Zhaoxin Fan](https://scholar.google.com/citations?user=JHvyYDQAAAAJ), 
[Hao Zhao ‚Ä†](https://scholar.google.com/citations?hl=en&user=ygQznUQAAAAJ)

[![GitHub Repo Stars](https://img.shields.io/github/stars/yisuanwang/Ultraman?label=stars&logo=github&color=brightgreen)](https://github.com/yisuanwang/Ultraman) 
[\[üóÇÔ∏èProject Page\]](https://air-discover.github.io/Ultraman/) 
[![arXiv](https://img.shields.io/badge/arXiv-2403.12028-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2403.12028)

  - This work converts a single image of the human body into a lifelike 3D model!
</div>
</div>





## üëÄ MM & LMM
<!--COLING 2024 MMADÔºöMulti-modal Movie Audio Description -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">LREC-COLING 2024</div><img src='images/pub_mmad.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  MMADÔºöMulti-modal Movie Audio Description
</h1>

[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
**<font color="#fc8803">Junhao Chen</font>**, 
[Xiang Li](https://scholar.google.com/citations?user=_wyYvQsAAAAJ&hl=zh-CN), 
[Haidong Xin](https://xhd0728.github.io/), 
Chao Li,
[Sheng Zhou ‚Ä†](https://scholar.google.com/citations?user=Ss76nMwAAAAJ&hl=zh-CN), 
[Jiajun Bu](https://scholar.google.com/citations?user=OgZP2okAAAAJ&hl=en)

<!-- [![GitHub](https://img.shields.io/github/stars/Daria8976/MMAD?label=stars&logo=github&color=brightgreen)](https://github.com/Daria8976/MMAD) -->

<!-- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1cn4W7IlooDk5X9JXBvsENRtExKJShb98#scrollTo=F0LyDZnKoTuT)
[![arXiv](https://img.shields.io/badge/arXiv-2311.13562-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2311.13562) -->

[![GitHub Repo Stars](https://img.shields.io/github/stars/Daria8976/MMAD?label=stars&logo=github&color=brightgreen)](https://github.com/Daria8976/MMAD) 

  - This work has unlocked a whole new experience of watching movies for the visually impaired.
</div>
</div>


<!-- Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv 2023</div><img src='images/pub_soulstyler.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object
</h1>

**<font color="#fc8803">Junhao Chen</font>**, Peng Rong, Jingbo Sun, Chao Li ‚Ä†, Xiang Li, [Hongwu Lv](http://homepage.hrbeu.edu.cn/web/lvhongwu?locale=zh_CN) 

[![GitHub Repo Stars](https://img.shields.io/github/stars/yisuanwang/Soulstyler?label=stars&logo=github&color=brightgreen)](https://github.com/yisuanwang/Soulstyler) 
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1cn4W7IlooDk5X9JXBvsENRtExKJShb98#scrollTo=F0LyDZnKoTuT) 
[![arXiv](https://img.shields.io/badge/arXiv-2311.13562-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2311.13562)

  - This work enables fine-grained stylization of a single image through text-guidance!

</div>
</div>


## üéô NLP & LLM

<!-- ICANN 2023 Towards Energy-Efficient Sentiment Classification with Spiking Neural Networks -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICANN 2023</div><img src='images/pub_spike.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  Towards Energy-Efficient Sentiment Classification with Spiking Neural Networks
</h1>

**<font color="#fc8803">Junhao Chen</font>**, [Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), Jingbo Sun, Chao Li ‚Ä†

[\[üìúPaper\]](https://doi.org/10.1007/978-3-031-44204-9_43)

  - This work applies a pulsed neural network to a natural language sentiment categorization task, reaching the leading edge in terms of energy consumption.

<!-- [**Project**](https://speechresearch.github.io/fastspeech/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>

- FastSpeech is the first fully parallel end-to-end speech synthesis model.
- **Academic Impact**: This work is included by many famous speech synthesis open-source projects, such as [ESPNet ![](https://img.shields.io/github/stars/espnet/espnet?style=social)](https://github.com/espnet/espnet). Our work are promoted by more than 20 media and forums, such as [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s/UkFadiUBy-Ymn-zhJ95JcQ)„ÄÅ[InfoQ](https://www.infoq.cn/article/tvy7hnin8bjvlm6g0myu).
- **Industry Impact**: FastSpeech has been deployed in [Microsoft Azure TTS service](https://techcommunity.microsoft.com/t5/azure-ai/neural-text-to-speech-extends-support-to-15-more-languages-with/ba-p/1505911) and supports 49 more languages with state-of-the-art AI quality. It was also shown as a text-to-speech system acceleration example in [NVIDIA GTC2020](https://resources.nvidia.com/events/GTC2020s21420). -->

</div>
</div>

<!-- EMNLP 2023 ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2023</div><img src='images/pub_zhujiu.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models
</h1>

[Baoli Zhang \*](https://scholar.google.com/citations?hl=en&user=ZUW0UbgAAAAJ), Haining Xie \*, Pengfan Du, **<font color="#fc8803">Junhao Chen</font>**, [Pengfei Cao](https://cpf-nlpr.github.io/), [Yubo Chen ‚Ä†](https://people.ucas.ac.cn/~yubochen), Shengping Liu, [Kang Liu](https://people.ucas.ac.cn/~liukang), [Jun Zhao](https://people.ucas.ac.cn/~zhaojun)

[\[üóÇÔ∏èProject Page\]](http://www.zhujiu-benchmark.com/introduction)
[\[üèÜLeaderboard \]](http://www.zhujiu-benchmark.com/leaderboard)
[![arXiv](https://img.shields.io/badge/arXiv-2308.14353-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2308.14353)
[\[üìúPaper\]](https://aclanthology.org/2023.emnlp-demo.44/)
[\[üé•Video\]](https://www.youtube.com/watch?v=qypkJ89L1Ic)

  - This work serves as a benchmark for evaluating the Chinese language capabilities of large language models.
</div>
</div>


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2022</div><img src='images/diffsinger.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism](https://arxiv.org/abs/2105.02446) \\
Jinglin Liu, Chengxi Li, **Yi Ren**, Feiyang Chen, Zhou Zhao

- Many [video demos](https://www.bilibili.com/video/BV1be411N7JA) created by the [DiffSinger community](https://github.com/openvpi) are released.
- DiffSinger was introduced in [a very popular video](https://www.bilibili.com/video/BV1uM411t7ZJ) (1600k+ views) on Bilibili!

- [**Project**](https://diffsinger.github.io/) \| [![](https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&label=DiffSpeech Stars)](https://github.com/NATSpeech/NATSpeech) \| [![](https://img.shields.io/github/stars/MoonInTheRiver/DiffSinger?style=social&label=DiffSinger Stars)](https://github.com/MoonInTheRiver/DiffSinger) \| [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo)](https://huggingface.co/spaces/NATSpeech/DiffSpeech)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2021</div><img src='images/portaspeech.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[PortaSpeech: Portable and High-Quality Generative Text-to-Speech](https://arxiv.org/abs/2109.15166) \\
**Yi Ren**, Jinglin Liu, Zhou Zhao

[**Project**](https://portaspeech.github.io/) \| [![](https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&label=Code+Stars)](https://github.com/NATSpeech/NATSpeech) \| [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo)](https://huggingface.co/spaces/NATSpeech/PortaSpeech)
</div>
</div> -->

## üìÉPatents

| Id | Date     | Name                                           | Number     | Type         |
| ---| -------- | ---------------------------------------------- | --- | -------------- | 
| 6  | 2024-03-22 |  ‰∏ÄÁßçÁõ≤‰∫∫Êô∫ËÉΩÁúºÈïú                             | - | China Utility Model |
| 5  | 2024-03-22 |  ‰∏ÄÁßçÊãºÊé•ÊëÑÂÉèÂ§¥                               | CN220647706U | China Utility Model |
| 4  | 2023-06-21 |  ‰∏ÄÁßçÂü∫‰∫éÁ•ûÁªèÁΩëÁªúÁöÑÊú¨Âú∞ÂåñÁªèÈ™åÂ®ÅËÉÅÂàÜÊûêÊñπÊ≥ïÂèäË£ÖÁΩÆ | CN116962012A | China Invention Publication     |
| 3  | 2023-06-21 |  ‰∏ÄÁßçÂü∫‰∫éÈù¢ÂêëÊµÅÂàáÂàÜÊäÄÊúØÁöÑÁΩëÁªúÊó•ÂøóËß£ÊûêÊñπÊ≥ïÂèäË£ÖÁΩÆ | CN116668154A | China Invention Publication     |
| 2  | 2022-10-31 |  ‰∏ÄÁßçÊï∞ÊéßÊú∫Â∫äÁöÑÂ∫üÊñôÂõûË£ÖÁΩÆ                       | CN218312317U | China Utility Model | 
| 1  | 2021-12-27 |  ‰∏ÄÁßçËà∞ËàπÁî®È´òÂàÜËæ®ÁéáË∂ÖËøúË∑ùÂÖ®ÊôØÊëÑÂÉèÂ§¥             | CN216565344U | China Utility Model | 


<!-- invention publicationÔºöÂèëÊòé‰∏ìÂà©Áî≥ËØ∑
invention grantÔºöÂèëÊòé‰∏ìÂà©ÊéàÊùÉ
utility modelÔºöÂÆûÁî®Êñ∞Âûã‰∏ìÂà©ÊéàÊùÉ
designÔºöÂ§ñËßÇËÆæËÆ°‰∏ìÂà©ÊéàÊùÉ
 -->
## üìÑSoftware copyrights

| Id | Date       | Name                             | Number        |
| ---- | ------------ | ------------------------------------- | --------------- |
| 7  | 2023-10-25 | TallybookÔºö‰∏ÄÊ¨æËÆ∞Ë¥¶ÁÆ°ÁêÜÁ≥ªÁªü                       | 2023SR1295186 |
| 6  | 2023-08-04 | Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÈ∏üÁ±ªÂ£∞Èü≥ËØÜÂà´Á≥ªÁªü                    | 2023SR0896658 |
| 5  | 2023-08-02 | Ê≥ï‰øù‚Äî‚Äî‰∏ìÊ≥®Â∞èÂæÆ‰ºÅ‰∏öÁöÑÊ≥ïÂæãÂí®ËØ¢Âπ≥Âè∞                | 2023SR0881467 |
| 4  | 2023-05-18 | ÁÅµÈ≠ÇÁîªÊâã‚Äî‚ÄîÊï∞Â≠óÁÖßÁâáËµÑ‰∫ß‰øÆÂ§ç‰∏éÁÆ°ÁêÜËΩØ‰ª∂iOS App     | 2023SR0551686 |
| 3  | 2023-05-18 | Soul PainterÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÂõæÂÉèÂ§ÑÁêÜÊäÄÊúØÁöÑAndroid App | 2023SR0551505 |
| 2  | 2022-05-26 | ËÆ°ÁÆóÊú∫ËßÜËßâÁõÆÊ†áÂÆö‰ΩçÊµãË∑ùÁ≥ªÁªü                        | 2022SR0649919 |
| 1  | 2022-05-26 | Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁâ©‰ΩìËØÜÂà´Â§ÑÁêÜÁ≥ªÁªü                    | 2022SR0649918 |
