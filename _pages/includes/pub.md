# üìù Publications 

## üßë‚Äçüé® Controllable World Model


<!-- LottieGPT -->
<div class='paper-box'>
  <div class='paper-box-image'>
      <div class="badge">CVPR 2026</div>
        <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
            onclick="window.open('https://dancetog.github.io/', '_blank');">
        <source src="images/pub_lottiegpt.mp4" type="video/mp4">
      </video>
  </div>

  <div class='paper-box-text' markdown="1">

  <h1 style="font-weight: bold">
    <a href="https://dancetog.github.io/" target="_blank" style="text-decoration: none; color: inherit;">
      LottieGPT: Tokenizing Vector Animation for Autoregressive Generation
    </a>
  </h1>

  [**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en), Kejun Gao, Yuehan Cui, [Mingze Sun](https://scholar.google.com/citations?user=TTW2mVoAAAAJ&hl=en), [Mingjin Chen](https://scholar.google.com/citations?user=uLfubbgAAAAJ&hl=en&oi=sra), Shaohui Wang, [Xiaoxiao Long](https://scholar.google.com/citations?hl=en&user=W3G5kZEAAAAJ), [Fei Ma](https://scholar.google.com/citations?user=RJOEAMYAAAAJ&hl=zh-CN), [Qi Tian](https://scholar.google.com/citations?hl=en&user=61b6eYkAAAAJ), [Hao Zhao ‚Ä†](https://scholar.google.com/citations?user=ygQznUQAAAAJ&hl=en), [Ruqi Huang ‚Ä†](https://scholar.google.com/citations?user=cgRY63gAAAAJ&hl=en)

  <div align="left">
  </div>

  - Tokenizes Lottie vector animations and finetunes a multimodal model to generate coherent, editable vector animations from text or visual prompts.


  </div>
</div>



<!-- HVG-3D -->
<div class='paper-box'>
  <div class='paper-box-image'>
      <div class="badge">CVPR 2026</div>
        <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
            onclick="window.open('https://dancetog.github.io/', '_blank');">
        <source src="images/pub_hvg3d.mp4" type="video/mp4">
      </video>
  </div>

  <div class='paper-box-text' markdown="1">

  <h1 style="font-weight: bold">
    <a href="https://dancetog.github.io/" target="_blank" style="text-decoration: none; color: inherit;">
      HVG-3D: Bridging Real and Simulation Domains for 3D-Conditional Hand-Object Interaction Video Synthesis
    </a>
  </h1>

  [Mingjin Chen](https://scholar.google.com/citations?user=uLfubbgAAAAJ&hl=en&oi=sra) *, [**<font color="#fc8803">Junhao Chen *</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en), [Zhaoxin Fan ‚Ä†](https://scholar.google.com/citations?user=JHvyYDQAAAAJ), Yujian Lee, Zichen Dang, [Lili Wang](https://scholar.google.com/citations?user=odVvHp4AAAAJ&hl=zh-CN), [Yawen Cui](https://openreview.net/profile?id=~Yawen_Cui1), [Lap-Pui Chau ‚Ä†](https://scholar.google.com/citations?user=MYREIH0AAAAJ&hl=en&oi=ao) , [Yi Wang](https://scholar.google.com.sg/citations?user=MAG909MAAAAJ&hl=en)

  <div align="left">
  </div>

  - HVG-3D: A 3D-aware HOI video diffusion framework with 3D ControlNet that turns one image plus 3D control signals into spatially precise, temporally coherent interaction videos.


  </div>
</div>


<!-- Animator-Centric Skeleton Generation -->
<div class='paper-box'>
  <div class='paper-box-image'>
      <div class="badge">CVPR 2026</div>
        <!-- <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
            onclick="window.open('https://dancetog.github.io/', '_blank');">
        <source src="images/pub_animator_ske_gen.png" type="video/mp4">
      </video> -->
      <img src='images/pub_animator_ske_gen.png' alt="sym" width="100%">
  </div>

  <div class='paper-box-text' markdown="1">

  <h1 style="font-weight: bold">
    <a href="https://dancetog.github.io/" target="_blank" style="text-decoration: none; color: inherit;">
      Animator-Centric Skeleton Generation on Objects with Fine-Grained Details
    </a>
  </h1>

  [Mingze Sun](https://scholar.google.com/citations?user=TTW2mVoAAAAJ&hl=en), [Cheng Zeng](https://scholar.google.com.hk/citations?user=9mPQzo4AAAAJ&hl=zh-CN), Jiansong Pei, [**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en), [Chaoyue Song](https://scholar.google.com/citations?user=4Yiz6gIAAAAJ&hl=en&oi=ao), Shaohui Wang, Tianyuan Chang, Bin Huang, Zijiao Zeng, [Ruqi Huang ‚Ä†](https://scholar.google.com/citations?user=cgRY63gAAAAJ&hl=en)

  <div align="left">
  </div>

  - Uses semantic-aware tokenization, a large rigged-mesh corpus, and a density-control module to generate high-quality, controllable skeletons for complex 3D assets.

  </div>
</div>

<!-- GarmentGPT -->
<div class='paper-box'>
  <div class='paper-box-image'>
      <div class="badge">ICLR 2026</div>
        <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
            onclick="window.open('https://openreview.net/forum?id=XzXKnazRBF', '_blank');">
        <source src="images/pub_garmentgpt.mp4" type="video/mp4">
      </video>
  </div>

  <div class='paper-box-text' markdown="1">

  <h1 style="font-weight: bold">
    <a href="https://openreview.net/forum?id=XzXKnazRBF" target="_blank" style="text-decoration: none; color: inherit;">
      GarmentGPT: Compositional Garment Pattern Generation via Discrete Latent Tokenization
    </a>
  </h1>

  Fangsheng Weng *, [**<font color="#fc8803">Junhao Chen *</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en), [Xiang Li](https://scholar.google.com/citations?user=COJdIx4AAAAJ), Jie Qin, Hanzhong Guo, Shaochun Hao, [Xiaoguang Han ‚Ä†](https://scholar.google.com/citations?user=z-rqsR4AAAAJ&hl=zh-CN)

  <div align="left">
  </div>
  [\[üìúPaper\]](https://openreview.net/forum?id=XzXKnazRBF)
  

  - Uses RVQ-VAE tokenization and a VLM generator to produce garment sewing patterns from discrete latent tokens, achieving strong accuracy on large curated datasets.


  </div>
</div>

<!-- From Frames to Sequences -->
<div class='paper-box'>
  <div class='paper-box-image'>
      <div class="badge">arxiv 2026</div>
        <!-- <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
            onclick="window.open('https://xingy038.github.io/F2S/', '_blank');">
        <source src="images/pub_from_frames.png" type="video/mp4">
      </video> -->
      <img src='images/pub_from_frames.png' alt="sym" width="100%">
  </div>

  <div class='paper-box-text' markdown="1">

  <h1 style="font-weight: bold">
    <a href="https://xingy038.github.io/F2S/" target="_blank" style="text-decoration: none; color: inherit;">
      From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction
    </a>
  </h1>

  [Xingyu Miao](https://scholar.google.com/citations?hl=en&user=QSY2OkIAAAAJ), [Junting Dong ‚Ä†](https://scholar.google.com/citations?user=dEzL5pAAAAAJ&hl=en), [Qin Zhao](https://scholar.google.com/citations?hl=en&user=c4OKn6IAAAAJ), [Yuhang Yang](https://scholar.google.com/citations?hl=en&user=x3aClGEAAAAJ), [**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en), [Yang Long ‚Ä†](https://scholar.google.com/citations?hl=en&user=IrkuknEAAAAJ)

  <div align="left">
    <a href="https://arxiv.org/abs/2602.01661"><img src="https://img.shields.io/badge/arXiv-PDF-b31b1b.svg?style=flat-square" alt="arXiv"></a>
  </div>

  - Learns temporally consistent human-centric segmentation, depth, and normals via synthetic video supervision and a two-stage static‚Üídynamic training pipeline.


  </div>
</div>

<!-- DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation -->
<div class='paper-box'>
  <div class='paper-box-image'>
      <div class="badge">ICLR 2026</div>
        <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
            onclick="window.open('https://dancetog.github.io/', '_blank');">
        <source src="images/pub_dancetog.mp4" type="video/mp4">
      </video>
  </div>

  <div class='paper-box-text' markdown="1">

  <h1 style="font-weight: bold">
    <a href="https://dancetog.github.io/" target="_blank" style="text-decoration: none; color: inherit;">
      <img src="images/dancetoglogo.png" alt="DanceTogether logo" style="height: 1em; vertical-align: -0.15em; margin-right: 0.3em;">
      <span class="gradient-text-dance">DanceTogether!</span> Identity-Preserving Multi-Person Interactive Video Generation
    </a>
  </h1>

  [**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?hl=en&user=uVMnzPMAAAAJ), 
  [Mingjin Chen](https://scholar.google.com/citations?user=uLfubbgAAAAJ&hl=en&oi=sra), 
  [Jianjin Xu](https://scholar.google.com/citations?hl=en&user=mTV0usAAAAAJ), 
  [Xiang Li](https://scholar.google.com/citations?user=_wyYvQsAAAAJ&hl=en&oi=sra), 
  [Junting Dong ‚Ä†](https://scholar.google.com/citations?user=dEzL5pAAAAAJ&hl=en),
  [Mingze Sun](https://scholar.google.com/citations?user=TTW2mVoAAAAJ&hl=en), 
  [Puhua Jiang](https://scholar.google.com/citations?user=E-k3WcgAAAAJ&hl=en), 
  [Hongxiang Li](https://scholar.google.com/citations?user=U4AwycUAAAAJ&hl=en&oi=ao), 
  [Yuhang Yang](https://scholar.google.com/citations?hl=en&user=x3aClGEAAAAJ),
  [Hao Zhao](https://scholar.google.com/citations?user=ygQznUQAAAAJ&hl=en), 
  [Xiaoxiao Long](https://scholar.google.com/citations?hl=en&user=W3G5kZEAAAAJ), 
  [Ruqi Huang ‚Ä†](https://scholar.google.com/citations?user=cgRY63gAAAAJ&hl=en)

  <div align="left">
    <a href="https://dancetog.github.io/"><img src="https://img.shields.io/static/v1?label=Homepage&message=DanceTogether&color=blue&logo=github-pages"></a> 
    <a href="https://github.com/yisuanwang/DanceTog"><img src="https://img.shields.io/github/stars/yisuanwang/DanceTog?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a> 
    <a href="https://arxiv.org/abs/2505.18078"><img src="https://img.shields.io/badge/arXiv-2505.18078-b31b1b.svg?style=flat-square" alt="arXiv"></a>
    <!-- <a href="https://huggingface.co/papers/2505.18078"><img src="https://img.shields.io/static/v1?label=Paper&message=HuggingFace&color=yellow"></a> 
    <a href="https://huggingface.co/spaces/BestWishYsh/ChronoMagic-Bench"><img src="https://img.shields.io/static/v1?label=LeaderBoard&message=HuggingFace&color=yellow"></a>  -->
  </div>

   - This work generates identity-preserving multi-person interactive dance videos with controllable motion and appearance!

  </div>
</div>

<!-- CVPR 2025 DRiVE -->
<div class='paper-box'>
  <div class='paper-box-image'>
      <div class="badge">CVPR 2025</div>
        <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
            onclick="window.open('https://driveavatar.github.io/', '_blank');">
        <source src="images/pub_DRiVE.mp4" type="video/mp4">
      </video>
  </div>

  <div class='paper-box-text' markdown="1">


  <h1 style="font-weight: bold">
    <a href="https://driveavatar.github.io/" target="_blank" style="text-decoration: none; color: inherit;">
      <span class="gradient-text-drive">DRiVE</span>:
      <span style="text-decoration: underline; text-decoration-skip-ink: none;">D</span>iffusion-based 
      <span style="text-decoration: underline; text-decoration-skip-ink: none;">Ri</span>gging Empowers Generation of 
      <span style="text-decoration: underline; text-decoration-skip-ink: none;">V</span>ersatile and 
      <span style="text-decoration: underline; text-decoration-skip-ink: none;">E</span>xpressive Characters
    </a>
  </h1>


  [Mingze Sun *](https://scholar.google.com/citations?hl=en&user=TTW2mVoAAAAJ), 
  [**<font color="#fc8803">Junhao Chen *</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
  [Junting Dong ‚Ä†](https://scholar.google.com/citations?user=dEzL5pAAAAAJ&hl=en&oi=ao), 
  [Yurun Chen](https://scholar.google.com/citations?user=k8fKlQ0AAAAJ&hl=en&oi=ao), [Xinyu Jiang](https://scholar.google.com/citations?hl=en&user=njfKRXQAAAAJ), Shiwei Mao,
  [Puhua Jiang](https://scholar.google.com/citations?user=E-k3WcgAAAAJ&hl=en), 
  [Jingbo Wang](https://scholar.google.com/citations?user=GStTsxAAAAAJ&hl=en), 
  [Bo Dai](https://scholar.google.com/citations?hl=en&user=KNWTvgEAAAAJ), 
  [Ruqi Huang ‚Ä†](https://scholar.google.com/citations?user=cgRY63gAAAAJ&hl=en)

  <div align="left">
    <!-- <a href="https://github.com/yisuanwang/DRiVE"><img src="https://img.shields.io/static/v1?label=Code&message=Github&color=blue&logo=github-pages"></a>  -->
    <a href="https://driveavatar.github.io/"><img src="https://img.shields.io/static/v1?label=Homepage&message=DRiVE&color=blue&logo=github-pages"></a> 
    <!-- <a href='https://driveavatar.github.io/'><img src='https://img.shields.io/badge/Project-Page-green'></a>  -->
    <!-- <a href="https://arxiv.org/pdf/2411.02293"><img src="https://img.shields.io/static/v1?label=Tech Report&message=Arxiv&color=red&logo=arxiv"></a>  -->
    <!-- <a href="https://huggingface.co/Tencent/Hunyuan3D-1"><img src="https://img.shields.io/static/v1?label=Checkpoints&message=HuggingFace&color=yellow"></a>  -->
    <!-- <a href="https://huggingface.co/spaces/Tencent/Hunyuan3D-1"><img src="https://img.shields.io/static/v1?label=Demo&message=HuggingFace&color=yellow"></a>  -->
    <a href="https://github.com/yisuanwang/DRiVE"><img src="https://img.shields.io/github/stars/yisuanwang/DRiVE?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a> 
    <a href="https://arxiv.org/abs/2411.17423"><img src="https://img.shields.io/badge/arXiv-2411.17423-b31b1b.svg?style=flat-square" alt="arXiv"></a>
  </div>

   - This work generates skeleton and skinning with clothes and hair for 3d gaussian avatar!

  </div>
</div>

<!-- MVA 2025, Ultraman -->
<div class='paper-box'>
  <div class='paper-box-image'>
      <div class="badge">Machine Vision and Applications 2026</div>
      <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
            onclick="window.open('https://air-discover.github.io/Ultraman/', '_blank');">
        <source src="images/pub_ultraman_Compressed.mp4" type="video/mp4">
      </video>
  </div>

  <div class='paper-box-text' markdown="1">

  <h1 style="font-weight: bold">
    <a href="https://air-discover.github.io/Ultraman/" target="_blank">
      <b style="color: #5a3e91">Ultra</b><b style="color: #d73d5f">man</b>: Single Image 3D Human Reconstruction with Ultra Speed and Detail
    </a>
  </h1>

  [Mingjin Chen *](https://scholar.google.com/citations?user=uLfubbgAAAAJ&hl=en&oi=sra), 
  [**<font color="#fc8803">Junhao Chen *</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
  [Huan-ang Gao](https://scholar.google.com/citations?hl=en&user=WvbKfLgAAAAJ), 
  [Xiaoxue Chen](https://scholar.google.com/citations?hl=en&user=_tz64W0AAAAJ), 
  [Zhaoxin Fan](https://scholar.google.com/citations?user=JHvyYDQAAAAJ), 
  [Hao Zhao ‚Ä†](https://scholar.google.com/citations?hl=en&user=ygQznUQAAAAJ)


  <a href="https://air-discover.github.io/Ultraman/"><img src="https://img.shields.io/static/v1?label=Homepage&message=Ultraman&color=blue&logo=github-pages"></a> 
  [![GitHub Repo Stars](https://img.shields.io/github/stars/tomorrow1238/Ultraman?label=stars&logo=github&color=brightgreen)](https://github.com/tomorrow1238/Ultraman) 
  [![arXiv](https://img.shields.io/badge/arXiv-2403.12028-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2403.12028)

   - This work converts a single image of the human body into a lifelike 3D model!
  </div>
</div>

<!-- COLING 2025, Idea23D -->
<div class='paper-box'>
  <div class='paper-box-image'>
      <div class="badge">COLING 2025</div>
      <video autoplay class="video-style" loop muted playsinline poster="images/spinner.svg" width="100%" 
            onclick="window.open('https://idea23d.github.io/', '_blank');">
        <source src="images/pub_idea23d.mp4" type="video/mp4">
      </video>
  </div>
  <div class='paper-box-text' markdown="1">
  <h1 style="font-weight: bold">
    <a href="https://idea23d.github.io/" target="_blank">
      <span class="gradient-text-idea23d">Idea23D</span>: Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs
    </a>
  </h1>


  [**<font color="#fc8803">Junhao Chen *</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
  [Xiang Li *](https://scholar.google.com/citations?user=_wyYvQsAAAAJ&hl=zh-CN), 
  [Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
  Chao Li, 
  [Zhaoxin Fan ‚Ä†](https://scholar.google.com/citations?user=JHvyYDQAAAAJ), 
  [Hao Zhao ‚Ä†](https://scholar.google.com/citations?hl=en&user=ygQznUQAAAAJ)

  <!-- <a href='https://idea23d.github.io/'><img src='https://img.shields.io/badge/Project-Page-green'></a>
  [![GitHub Repo Stars](https://img.shields.io/github/stars/yisuanwang/Idea23D?label=stars&logo=github&color=brightgreen)](https://github.com/yisuanwang/Idea23D)
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1u_lJRvxIlBUPjC_Lou57SWLEnc5vLgQ6?usp=sharing)
  [![arXiv](https://img.shields.io/badge/arXiv-2404.04363-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2404.04363) -->

  <div align="left">
    <!-- <a href='https://idea23d.github.io/'>
      <img src='https://img.shields.io/badge/Project-Page-green' alt="Project Page">
    </a> -->
    <a href="https://idea23d.github.io/"><img src="https://img.shields.io/static/v1?label=Homepage&message=Idea23D&color=blue&logo=github-pages"></a> 
    <a href="https://github.com/yisuanwang/Idea23D"><img src="https://img.shields.io/github/stars/yisuanwang/Idea23D?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a> 
    <a href="https://colab.research.google.com/drive/1u_lJRvxIlBUPjC_Lou57SWLEnc5vLgQ6?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a> 
    <a href="https://arxiv.org/abs/2404.04363"><img src="https://img.shields.io/badge/arXiv-2404.04363-b31b1b.svg?style=flat-square" alt="arXiv"></a> 
  </div>
   - This work enables automated 3D model design and generation for people!

  </div>
</div>


## üéô Multi-modal

<!-- COLING 2024 MMADÔºöMulti-modal Movie Audio Description -->
<div class="paper-box">
    <!-- Paper Box Image Section -->
    <div class="paper-box-image">
      <div>
        <div class="badge">COLING 2024</div>
        <!-- Clickable Image with Link -->
        <img src="images/pub_mmad.png" alt="sym" width="100%" 
            style="cursor: pointer;" 
            onclick="window.open('https://daria8976.github.io/mmad-page/', '_blank');">
      </div>
    </div>
  <div class='paper-box-text' markdown="1">
  <h1 style="font-weight: bold">
    <a href="https://daria8976.github.io/mmad-page/" target="_blank">
      <span class="gradient-text-MMAD">MMAD</span>:
        Multi-modal Movie Audio Description
    </a>
  </h1>


  [Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
  [**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
  [Xiang Li](https://scholar.google.com/citations?user=_wyYvQsAAAAJ&hl=zh-CN), 
  [Haidong Xin](https://xhd0728.github.io/), 
  Chao Li,
  [Sheng Zhou ‚Ä†](https://scholar.google.com/citations?user=Ss76nMwAAAAJ&hl=zh-CN), 
  [Jiajun Bu](https://scholar.google.com/citations?user=OgZP2okAAAAJ&hl=en)


  <a href="https://daria8976.github.io/mmad-page/"><img src="https://img.shields.io/static/v1?label=Homepage&message=MMAD&color=blue&logo=github-pages"></a> 
  [![GitHub Repo Stars](https://img.shields.io/github/stars/Daria8976/MMAD?label=stars&logo=github&color=brightgreen)](https://github.com/Daria8976/MMAD) 
  [\[üìúPaper\]](https://aclanthology.org/2024.lrec-main.998/)

   - This work has unlocked a whole new experience of watching movies for the visually impaired.
  </div>
</div>

<!-- Soulstyler -->
<div class='paper-box'>
  <div class='paper-box-image'><div><div class="badge">arxiv 2023</div>
  <img src='images/pub_soulstyler.jpg' alt="sym" width="100%">
  </div></div>
  <div class='paper-box-text' markdown="1">

  <h1 style="font-weight: bold">
    FineStyler: Text-guided Instance-level Fine-grained Image Style Transfer
  </h1>

  [**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
  Rong Peng, 
  Xiang Li, 
  [Jingbo Sun](https://scholar.google.com/citations?hl=en&user=DowEKzcAAAAJ),
  [Hao Zhao](https://scholar.google.com/citations?user=ygQznUQAAAAJ&hl=en), 
  [Ruqi Huang](https://scholar.google.com/citations?user=cgRY63gAAAAJ&hl=en)

  [![GitHub Repo Stars](https://img.shields.io/github/stars/yisuanwang/Soulstyler?label=stars&logo=github&color=brightgreen)](https://github.com/yisuanwang/Soulstyler) 
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1cn4W7IlooDk5X9JXBvsENRtExKJShb98#scrollTo=F0LyDZnKoTuT) 
  [![arXiv](https://img.shields.io/badge/arXiv-2311.13562-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2311.13562)

   - This work enables fine-grained stylization of a single image through text-guidance!

  </div>
</div>

## üëÄ Large Language Model

<!--EMNLP 2025, LLMsPark -->
<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">EMNLP 2025</div>
      <img src="images/pub_llmspark.jpg" alt="sym" width="100%" 
           style="cursor: pointer;" 
           onclick="window.open('https://llmsparks.github.io/', '_blank');">
    </div>
  </div>
  <div class='paper-box-text' markdown="1">
  <h1 style="font-weight: bold">
    <a href="https://llmsparks.github.io/" target="_blank">
      LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts
    </a>
  </h1>

  <!-- author list -->
  [**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
  [Jingbo Sun](https://scholar.google.com/citations?hl=en&user=DowEKzcAAAAJ),
  Xiang Li, 
  Haidong Xin, 
  Yuhao Xue, 
  Yibin Xu, 
  [Hao Zhao ‚Ä†](https://scholar.google.com/citations?user=ygQznUQAAAAJ&hl=en)

  <!-- links -->
  <a href="https://llmsparks.github.io/"><img src="https://img.shields.io/static/v1?label=Homepage&message=LLMsPark&color=blue&logo=github-pages"></a> 
  <!-- [![GitHub Repo Stars](https://img.shields.io/github/stars/HC-Guo/IWBench?label=stars&logo=github&color=brightgreen)](https://github.com/HC-Guo/IWBench)  -->
  <!-- [![arXiv](https://img.shields.io/badge/arXiv-2409.18980-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2409.18980) -->

  - This work evaluates LLMs through a game-theoretic framework.
  </div>
</div>


<!-- ACL 2025 IW-Bench -->
<div class='paper-box'>
    <div class='paper-box-image'>
      <div>
        <div class="badge">ACL 2025</div>
        <img src="images/pub_iwbench.svg" alt="sym" width="100%" 
            style="cursor: pointer;" 
            onclick="window.open('https://iw-bench-page.vercel.app/', '_blank');">
      </div>
    </div>

  <div class='paper-box-text' markdown="1">

  <h1 style="font-weight: bold">
    <a href="https://iw-bench-page.vercel.app/" target="_blank">
      IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web
    </a>
  </h1>


  [Hongcheng Guo](https://scholar.google.com/citations?hl=en&user=eynbo4cAAAAJ), 
  [Wei Zhang](https://scholar.google.com/citations?user=NaWMztYAAAAJ&hl=en&oi=sra), 
  [**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
  Yaonan Gu,
  [Jian Yang](https://scholar.google.com/citations?user=i9opWEgAAAAJ&hl=en), 
  [Junjia Du](https://scholar.google.com/citations?user=gC28ufYAAAAJ&hl=en&oi=sra), 
  [Shaosheng Cao](https://scholar.google.com/citations?user=ZF0ntl4AAAAJ&hl=en&oi=ao), 
  [Binyuan Hui](https://scholar.google.com/citations?user=RBb3ItMAAAAJ&hl=en&oi=sra), 
  [Tianyu Liu](https://scholar.google.com/citations?user=6hHbBwwAAAAJ&hl=en), 
  [Jianxin Ma](https://scholar.google.com/citations?hl=en&user=WdDFFlIAAAAJ), 
  [Chang Zhou](https://scholar.google.com/citations?hl=en&user=QeSoG3sAAAAJ),
  [Zhoujun Li](https://scholar.google.com/citations?user=e-4LoEcAAAAJ&hl=en&oi=ao)

  <a href="https://iw-bench-page.vercel.app/"><img src="https://img.shields.io/static/v1?label=Homepage&message=IW-bench&color=blue&logo=github-pages"></a> 
  [![GitHub Repo Stars](https://img.shields.io/github/stars/HC-Guo/IWBench?label=stars&logo=github&color=brightgreen)](https://github.com/HC-Guo/IWBench) 
  [![arXiv](https://img.shields.io/badge/arXiv-2409.18980-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2409.18980)

   - This work is a benchmark for evaluating MLLM image-2-html code generation capabilities.
  </div>
</div>

<!-- EMNLP 2023 ZhuJiu -->
<div class='paper-box'>
  <div class='paper-box-image'><div><div class="badge">EMNLP 2023</div><img src='images/pub_zhujiu.jpg' alt="sym" width="100%"></div></div>
  <div class='paper-box-text' markdown="1">

  <h1 style="font-weight: bold">
    ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models
  </h1>

  [Baoli Zhang](https://scholar.google.com/citations?hl=en&user=ZUW0UbgAAAAJ), Haining Xie, Pengfan Du,
  [**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
  [Pengfei Cao](https://cpf-nlpr.github.io/), [Yubo Chen ‚Ä†](https://people.ucas.ac.cn/~yubochen), Shengping Liu, [Kang Liu](https://people.ucas.ac.cn/~liukang), [Jun Zhao](https://people.ucas.ac.cn/~zhaojun)

  <a href="http://www.zhujiu-benchmark.com/introduction"><img src="https://img.shields.io/static/v1?label=Homepage&message=Zhujiu&color=blue&logo=github-pages"></a> 
  [\[üèÜLeaderboard \]](http://www.zhujiu-benchmark.com/leaderboard)
  [![arXiv](https://img.shields.io/badge/arXiv-2308.14353-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2308.14353)
  [\[üìúPaper\]](https://aclanthology.org/2023.emnlp-demo.44/)
  [\[üé•Video\]](https://www.youtube.com/watch?v=qypkJ89L1Ic)

   - This work serves as a benchmark for evaluating the Chinese language capabilities of large language models.
  </div>
</div>

<!-- ICANN 2023 Towards Energy-Efficient Sentiment Classification with Spiking Neural Networks -->
<div class='paper-box'>
  <div class='paper-box-image'><div><div class="badge">ICANN 2023</div><img src='images/pub_spike.png' alt="sym" width="100%"></div></div>
  <div class='paper-box-text' markdown="1">

  <h1 style="font-weight: bold">
    Towards Energy-Efficient Sentiment Classification with Spiking Neural Networks
  </h1>

  [**<font color="#fc8803">Junhao Chen</font>**](https://scholar.google.com/citations?user=uVMnzPMAAAAJ&hl=en),
  [Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), [Jingbo Sun](https://scholar.google.com/citations?hl=en&user=DowEKzcAAAAJ), Chao Li ‚Ä†

  [\[üìúPaper\]](https://doi.org/10.1007/978-3-031-44204-9_43)

  - This work applies a pulsed neural network to a natural language sentiment categorization task, reaching the leading edge in terms of energy consumption.

  </div>
</div>
