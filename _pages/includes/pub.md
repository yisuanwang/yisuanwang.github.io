
# ğŸ“ Publications 

## ğŸ§‘â€ğŸ¨ 3D AIGC


<!-- Idea23D -->
<div class='paper-box'>

<div class='paper-box-image'>
  <!-- <div class='video-container'> -->
    <div class="badge">arxiv 2024</div>
      <video autoplay="" class="video-style"  loop="" muted="" playsinline="" poster="images/spinner.svg" width="100%">
          <source src="images/pub_idea23d.mp4" type="video/mp4" >
      </video>
  <!-- </div> -->
</div>

<div class='paper-box-text' markdown="1">

<!-- <h1 style="font-weight: bold">
  <a href="https://air-discover.github.io/Idea-2-3D/" target="_blank">
    <b style="color: #5a3e91">Idea</b>-<b style="color: #17bfe9">2</b>-<b style="color: #d73d5f">3D</b>:
      Iterative Self-Refinement with Large Multimodal Model for Automated 3D Model Design and Generation
  </a>
</h1> -->

<h1 style="font-weight: bold">
  <a href="https://air-discover.github.io/Idea-2-3D/" target="_blank">
    <span class="gradient-text">Idea-2-3D</span>:
      Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs
  </a>
</h1>

<!-- <h1 style="font-weight: bold;">
  <a href="https://air-discover.github.io/Idea-2-3D/" target="_blank">
  <span class="gradient-text">Idea-2-3D</span>
  <span>
    : Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs
  </span>
  </a>
</h1> -->

**<font color="#fc8803">Junhao Chen *</font>**, 
[Xiang Li *](https://scholar.google.com/citations?user=_wyYvQsAAAAJ&hl=zh-CN), 
[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
Chao Li, 
[Zhaoxin Fan](https://scholar.google.com/citations?user=JHvyYDQAAAAJ), 
[Hao Zhao â€ ](https://scholar.google.com/citations?hl=en&user=ygQznUQAAAAJ)


[ğŸ—‚Project Page](https://air-discover.github.io/Idea-2-3D/) | [![GitHub Repo Stars](https://img.shields.io/github/stars/yisuanwang/Idea23D?label=stars&logo=github&color=brightgreen)](https://github.com/yisuanwang/Idea23D) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1u_lJRvxIlBUPjC_Lou57SWLEnc5vLgQ6?usp=sharing) | [![arXiv](https://img.shields.io/badge/arXiv-2404.04363-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2404.04363)


  - This work enables automated 3D model design and generation for people!

</div>
</div>


<!-- Ultraman -->
<div class='paper-box'>

<div class='paper-box-image'>
  <!-- <div class='video-container'> -->
    <div class="badge">arxiv 2024</div>
      <video autoplay="" class="video-style"  loop="" muted="" playsinline="" poster="images/spinner.svg" width="100%">
          <source src="images/pub_ultraman_Compressed.mp4" type="video/mp4" >
      </video>
  <!-- </div> -->
</div>

<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  <a href="https://air-discover.github.io/Ultraman/" target="_blank">
    <b style="color: #5a3e91">Ultra</b><b style="color: #d73d5f">man</b>: Single Image 3D Human Reconstruction with Ultra Speed and Detail
  </a>
</h1>
  
Mingjin Chen *, 
**<font color="#fc8803">Junhao Chen *</font>**, 
[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
[Huan-ang Gao](https://scholar.google.com/citations?hl=en&user=WvbKfLgAAAAJ), 
[Xiaoxue Chen](https://scholar.google.com/citations?hl=en&user=_tz64W0AAAAJ), 
[Zhaoxin Fan](https://scholar.google.com/citations?user=JHvyYDQAAAAJ), 
[Hao Zhao â€ ](https://scholar.google.com/citations?hl=en&user=ygQznUQAAAAJ)

[![GitHub Repo Stars](https://img.shields.io/github/stars/yisuanwang/Ultraman?label=stars&logo=github&color=brightgreen)](https://github.com/yisuanwang/Ultraman) 
[\[ğŸ—‚ï¸Project Page\]](https://air-discover.github.io/Ultraman/) 
[![arXiv](https://img.shields.io/badge/arXiv-2403.12028-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2403.12028)

  - This work converts a single image of the human body into a lifelike 3D model!
</div>
</div>





## ğŸ‘€ MM & LMM
<!--COLING 2024 MMADï¼šMulti-modal Movie Audio Description -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">LREC-COLING 2024</div><img src='images/pub_mmad.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  MMADï¼šMulti-modal Movie Audio Description
</h1>

[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), 
**<font color="#fc8803">Junhao Chen</font>**, 
[Xiang Li](https://scholar.google.com/citations?user=_wyYvQsAAAAJ&hl=zh-CN), 
[Haidong Xin](https://xhd0728.github.io/), 
Chao Li,
[Sheng Zhou â€ ](https://scholar.google.com/citations?user=Ss76nMwAAAAJ&hl=zh-CN), 
[Jiajun Bu](https://scholar.google.com/citations?user=OgZP2okAAAAJ&hl=en)

<!-- [![GitHub](https://img.shields.io/github/stars/Daria8976/MMAD?label=stars&logo=github&color=brightgreen)](https://github.com/Daria8976/MMAD) -->

<!-- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1cn4W7IlooDk5X9JXBvsENRtExKJShb98#scrollTo=F0LyDZnKoTuT)
[![arXiv](https://img.shields.io/badge/arXiv-2311.13562-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2311.13562) -->

[![GitHub Repo Stars](https://img.shields.io/github/stars/Daria8976/MMAD?label=stars&logo=github&color=brightgreen)](https://github.com/Daria8976/MMAD) 

  - This work has unlocked a whole new experience of watching movies for the visually impaired.
</div>
</div>


<!-- Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv 2023</div><img src='images/pub_soulstyler.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object
</h1>

**<font color="#fc8803">Junhao Chen</font>**, Peng Rong, Jingbo Sun, Chao Li â€ , Xiang Li, [Hongwu Lv](http://homepage.hrbeu.edu.cn/web/lvhongwu?locale=zh_CN) 

[![GitHub Repo Stars](https://img.shields.io/github/stars/yisuanwang/Soulstyler?label=stars&logo=github&color=brightgreen)](https://github.com/yisuanwang/Soulstyler) 
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1cn4W7IlooDk5X9JXBvsENRtExKJShb98#scrollTo=F0LyDZnKoTuT) 
[![arXiv](https://img.shields.io/badge/arXiv-2311.13562-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2311.13562)

  - This work enables fine-grained stylization of a single image through text-guidance!

</div>
</div>


## ğŸ™ NLP & LLM

<!-- ICANN 2023 Towards Energy-Efficient Sentiment Classification with Spiking Neural Networks -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICANN 2023</div><img src='images/pub_spike.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  Towards Energy-Efficient Sentiment Classification with Spiking Neural Networks
</h1>

**<font color="#fc8803">Junhao Chen</font>**, [Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ&hl=en), Jingbo Sun, Chao Li â€ 

[\[ğŸ“œPaper\]](https://doi.org/10.1007/978-3-031-44204-9_43)

  - This work applies a pulsed neural network to a natural language sentiment categorization task, reaching the leading edge in terms of energy consumption.

<!-- [**Project**](https://speechresearch.github.io/fastspeech/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>

- FastSpeech is the first fully parallel end-to-end speech synthesis model.
- **Academic Impact**: This work is included by many famous speech synthesis open-source projects, such as [ESPNet ![](https://img.shields.io/github/stars/espnet/espnet?style=social)](https://github.com/espnet/espnet). Our work are promoted by more than 20 media and forums, such as [æœºå™¨ä¹‹å¿ƒ](https://mp.weixin.qq.com/s/UkFadiUBy-Ymn-zhJ95JcQ)ã€[InfoQ](https://www.infoq.cn/article/tvy7hnin8bjvlm6g0myu).
- **Industry Impact**: FastSpeech has been deployed in [Microsoft Azure TTS service](https://techcommunity.microsoft.com/t5/azure-ai/neural-text-to-speech-extends-support-to-15-more-languages-with/ba-p/1505911) and supports 49 more languages with state-of-the-art AI quality. It was also shown as a text-to-speech system acceleration example in [NVIDIA GTC2020](https://resources.nvidia.com/events/GTC2020s21420). -->

</div>
</div>

<!-- EMNLP 2023 ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2023</div><img src='images/pub_zhujiu.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<h1 style="font-weight: bold">
  ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models
</h1>

[Baoli Zhang \*](https://scholar.google.com/citations?hl=en&user=ZUW0UbgAAAAJ), Haining Xie \*, Pengfan Du, **<font color="#fc8803">Junhao Chen</font>**, [Pengfei Cao](https://cpf-nlpr.github.io/), [Yubo Chen â€ ](https://people.ucas.ac.cn/~yubochen), Shengping Liu, [Kang Liu](https://people.ucas.ac.cn/~liukang), [Jun Zhao](https://people.ucas.ac.cn/~zhaojun)

[\[ğŸ—‚ï¸Project Page\]](http://www.zhujiu-benchmark.com/introduction)
[\[ğŸ†Leaderboard \]](http://www.zhujiu-benchmark.com/leaderboard)
[![arXiv](https://img.shields.io/badge/arXiv-2308.14353-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2308.14353)
[\[ğŸ“œPaper\]](https://aclanthology.org/2023.emnlp-demo.44/)
[\[ğŸ¥Video\]](https://www.youtube.com/watch?v=qypkJ89L1Ic)

  - This work serves as a benchmark for evaluating the Chinese language capabilities of large language models.
</div>
</div>


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2022</div><img src='images/diffsinger.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism](https://arxiv.org/abs/2105.02446) \\
Jinglin Liu, Chengxi Li, **Yi Ren**, Feiyang Chen, Zhou Zhao

- Many [video demos](https://www.bilibili.com/video/BV1be411N7JA) created by the [DiffSinger community](https://github.com/openvpi) are released.
- DiffSinger was introduced in [a very popular video](https://www.bilibili.com/video/BV1uM411t7ZJ) (1600k+ views) on Bilibili!

- [**Project**](https://diffsinger.github.io/) \| [![](https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&label=DiffSpeech Stars)](https://github.com/NATSpeech/NATSpeech) \| [![](https://img.shields.io/github/stars/MoonInTheRiver/DiffSinger?style=social&label=DiffSinger Stars)](https://github.com/MoonInTheRiver/DiffSinger) \| [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo)](https://huggingface.co/spaces/NATSpeech/DiffSpeech)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2021</div><img src='images/portaspeech.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[PortaSpeech: Portable and High-Quality Generative Text-to-Speech](https://arxiv.org/abs/2109.15166) \\
**Yi Ren**, Jinglin Liu, Zhou Zhao

[**Project**](https://portaspeech.github.io/) \| [![](https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&label=Code+Stars)](https://github.com/NATSpeech/NATSpeech) \| [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo)](https://huggingface.co/spaces/NATSpeech/PortaSpeech)
</div>
</div> -->

## ğŸ“ƒPatents

| Id | Date     | Name                                           | Number     | Type         |
| ---| -------- | ---------------------------------------------- | --- | -------------- | 
| 6  | 2024-03-22 |  ä¸€ç§ç›²äººæ™ºèƒ½çœ¼é•œ                             | - | China Utility Model |
| 5  | 2024-03-22 |  ä¸€ç§æ‹¼æ¥æ‘„åƒå¤´                               | CN220647706U | China Utility Model |
| 4  | 2023-06-21 |  ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„æœ¬åœ°åŒ–ç»éªŒå¨èƒåˆ†ææ–¹æ³•åŠè£…ç½® | CN116962012A | China Invention Publication     |
| 3  | 2023-06-21 |  ä¸€ç§åŸºäºé¢å‘æµåˆ‡åˆ†æŠ€æœ¯çš„ç½‘ç»œæ—¥å¿—è§£ææ–¹æ³•åŠè£…ç½® | CN116668154A | China Invention Publication     |
| 2  | 2022-10-31 |  ä¸€ç§æ•°æ§æœºåºŠçš„åºŸæ–™å›è£…ç½®                       | CN218312317U | China Utility Model | 
| 1  | 2021-12-27 |  ä¸€ç§èˆ°èˆ¹ç”¨é«˜åˆ†è¾¨ç‡è¶…è¿œè·å…¨æ™¯æ‘„åƒå¤´             | CN216565344U | China Utility Model | 


<!-- invention publicationï¼šå‘æ˜ä¸“åˆ©ç”³è¯·
invention grantï¼šå‘æ˜ä¸“åˆ©æˆæƒ
utility modelï¼šå®ç”¨æ–°å‹ä¸“åˆ©æˆæƒ
designï¼šå¤–è§‚è®¾è®¡ä¸“åˆ©æˆæƒ
 -->
## ğŸ“„Software copyrights

| Id | Date       | Name                             | Number        |
| ---- | ------------ | ------------------------------------- | --------------- |
| 7  | 2023-10-25 | Tallybookï¼šä¸€æ¬¾è®°è´¦ç®¡ç†ç³»ç»Ÿ                       | 2023SR1295186 |
| 6  | 2023-08-04 | åŸºäºæ·±åº¦å­¦ä¹ çš„é¸Ÿç±»å£°éŸ³è¯†åˆ«ç³»ç»Ÿ                    | 2023SR0896658 |
| 5  | 2023-08-02 | æ³•ä¿â€”â€”ä¸“æ³¨å°å¾®ä¼ä¸šçš„æ³•å¾‹å’¨è¯¢å¹³å°                | 2023SR0881467 |
| 4  | 2023-05-18 | çµé­‚ç”»æ‰‹â€”â€”æ•°å­—ç…§ç‰‡èµ„äº§ä¿®å¤ä¸ç®¡ç†è½¯ä»¶iOS App     | 2023SR0551686 |
| 3  | 2023-05-18 | Soul PainteråŸºäºæ·±åº¦å­¦ä¹ å›¾åƒå¤„ç†æŠ€æœ¯çš„Android App | 2023SR0551505 |
| 2  | 2022-05-26 | è®¡ç®—æœºè§†è§‰ç›®æ ‡å®šä½æµ‹è·ç³»ç»Ÿ                        | 2022SR0649919 |
| 1  | 2022-05-26 | åŸºäºæ·±åº¦å­¦ä¹ çš„ç‰©ä½“è¯†åˆ«å¤„ç†ç³»ç»Ÿ                    | 2022SR0649918 |
